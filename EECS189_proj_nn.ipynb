{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "path = '/home/han/repos/BEACO2N-CO2-Correction/data/'\n",
    "X_train  = torch.Tensor(np.load(path + 'X_train.npy', allow_pickle=True))\n",
    "X_test   = torch.Tensor(np.load(path + 'X_test.npy', allow_pickle=True))\n",
    "X_val    = torch.Tensor(np.load(path + 'X_val.npy', allow_pickle=True))\n",
    "y_train  = torch.Tensor(np.load(path + 'y_train.npy', allow_pickle=True))\n",
    "y_test   = torch.Tensor(np.load(path + 'y_test.npy', allow_pickle=True))\n",
    "y_val    = torch.Tensor(np.load(path + 'y_val.npy', allow_pickle=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "n_iters      = 50\n",
    "nsize_in     = X_train.shape[1] \n",
    "nsize_out    = 1\n",
    "nsize_hidden = X_train.shape[1]\n",
    "n_layers     = 4\n",
    "activations = [nn.Sigmoid(),#nn.ReLU(),\n",
    "               nn.Sigmoid(),#nn.ReLU(),\n",
    "               nn.Sigmoid(),#nn.ReLU(), \n",
    "               None]\n",
    "layer_combiners = [nn.Linear(nsize_in, nsize_hidden),\n",
    "                   nn.Linear(nsize_in, nsize_hidden-1),\n",
    "                   nn.Linear(nsize_hidden-1, nsize_hidden-2),\n",
    "                   nn.Linear(nsize_hidden-2, nsize_out)]\n",
    "learning_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(n_layers, activation, combiners):\n",
    "    \n",
    "    if len(activation) != n_layers:\n",
    "        act = activation[0]*(n_layers - 1)\n",
    "        act.append(activation[-1])\n",
    "    else:\n",
    "        act = activations\n",
    "    if len(combiners) != n_layers:\n",
    "        combine = combiners[0]*(n_layers - 1)\n",
    "        combine.append(combiners[-1])\n",
    "    else:\n",
    "        combine = combiners\n",
    "        \n",
    "    for layer in range(n_layers):\n",
    "        if layer == 0:\n",
    "            model = nn.Sequential(combine[layer])\n",
    "            if act[layer]:\n",
    "                model.add_module(name=str(2*layer+1), module=act[layer])\n",
    "        else:\n",
    "            model.add_module(name=str(2*layer),module=combine[layer])\n",
    "            if act[layer]:\n",
    "                model.add_module(name=str(2*layer+1), module=act[layer])\n",
    "                \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    del model\n",
    "except:\n",
    "    pass\n",
    "model = create_model(n_layers, activations, layer_combiners)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(),lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(n_iters, lr=0.1,restart=False, pr=True):\n",
    "    losses\n",
    "    if restart:\n",
    "        try:\n",
    "            del model\n",
    "        except:\n",
    "            pass\n",
    "        model = create_model(n_layers, activations, layer_combiners)\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.SGD(model.parameters(),lr=lr)\n",
    "    for epoch in range(n_iters):\n",
    "       # Forward propagation\n",
    "        y_pred = model(X_train)\n",
    "\n",
    "        #Compute and print loss\n",
    "        loss  = criterion(y_pred, y_train)\n",
    "        losses[epoch] = loss.item()\n",
    "        if pr:\n",
    "            print('epoch: ',epoch, ' loss: ', loss.item())\n",
    "        if torch.isnan(loss):\n",
    "            break\n",
    "\n",
    "        #Zero the gradients (so that gradients don't accumulate from epoch to epoch)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #perform a backward pass (backpropagation)\n",
    "        loss.backward()\n",
    "\n",
    "        # Update the parameters\n",
    "        optimizer.step()\n",
    "    return lo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Loss:  tensor(443.4062, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/han/.local/lib/python3.8/site-packages/torch/nn/modules/loss.py:445: UserWarning: Using a target size (torch.Size([6327])) that is different to the input size (torch.Size([18981, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    }
   ],
   "source": [
    "# Lets Evaluate\n",
    "model.eval()\n",
    "print(model.training ) #Just to be sure\n",
    "\n",
    "ypred = model(X_test)\n",
    "loss  = criterion(y_pred, y_test)\n",
    "print(\"Loss: \", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([413.0459, 416.9233, 412.2977, 490.5103, 411.9405, 413.0804, 418.6821,\n",
      "        436.1230, 414.5334])\n",
      "tensor([[423.5479],\n",
      "        [423.5479],\n",
      "        [423.5479],\n",
      "        [423.5479],\n",
      "        [423.5479],\n",
      "        [423.5479],\n",
      "        [423.5479],\n",
      "        [423.5479],\n",
      "        [423.5479]], grad_fn=<SliceBackward>)\n",
      "tensor(134256.1250, grad_fn=<NormBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "ypred = model(X_test)\n",
    "err = torch.norm(ypred-y_test, p=2)\n",
    "print(y_test[1:10])\n",
    "print(ypred[1:10])\n",
    "print(err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([433.2132, 410.1967, 465.3893, 432.4657, 417.1220, 416.7152, 413.6294,\n",
       "        413.1808, 420.4663])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Lets search for best learning rate\n",
    "learning_rates = [0.0001, 0.001, 0.01, 0.1, 0.5, 1, 2, 10, 100]\n",
    "for lr in learning_rates:\n",
    "    run(200)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "415.7703552246094"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['T_destination',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__delitem__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattr__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__iter__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_apply',\n",
       " '_backward_hooks',\n",
       " '_buffers',\n",
       " '_call_impl',\n",
       " '_forward_hooks',\n",
       " '_forward_pre_hooks',\n",
       " '_forward_unimplemented',\n",
       " '_get_item_by_idx',\n",
       " '_get_name',\n",
       " '_load_from_state_dict',\n",
       " '_load_state_dict_pre_hooks',\n",
       " '_modules',\n",
       " '_named_members',\n",
       " '_non_persistent_buffers_set',\n",
       " '_parameters',\n",
       " '_register_load_state_dict_pre_hook',\n",
       " '_register_state_dict_hook',\n",
       " '_replicate_for_data_parallel',\n",
       " '_save_to_state_dict',\n",
       " '_slow_forward',\n",
       " '_state_dict_hooks',\n",
       " '_version',\n",
       " 'add_module',\n",
       " 'apply',\n",
       " 'bfloat16',\n",
       " 'buffers',\n",
       " 'children',\n",
       " 'cpu',\n",
       " 'cuda',\n",
       " 'double',\n",
       " 'dump_patches',\n",
       " 'eval',\n",
       " 'extra_repr',\n",
       " 'float',\n",
       " 'forward',\n",
       " 'half',\n",
       " 'load_state_dict',\n",
       " 'modules',\n",
       " 'named_buffers',\n",
       " 'named_children',\n",
       " 'named_modules',\n",
       " 'named_parameters',\n",
       " 'parameters',\n",
       " 'register_backward_hook',\n",
       " 'register_buffer',\n",
       " 'register_forward_hook',\n",
       " 'register_forward_pre_hook',\n",
       " 'register_parameter',\n",
       " 'requires_grad_',\n",
       " 'share_memory',\n",
       " 'state_dict',\n",
       " 'to',\n",
       " 'train',\n",
       " 'training',\n",
       " 'type',\n",
       " 'zero_grad']"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Module.share_memory of Sequential(\n",
       "  (0): Linear(in_features=7, out_features=7, bias=True)\n",
       "  (1): Sigmoid()\n",
       "  (2): Linear(in_features=7, out_features=6, bias=True)\n",
       "  (3): Sigmoid()\n",
       "  (4): Linear(in_features=6, out_features=5, bias=True)\n",
       "  (5): Sigmoid()\n",
       "  (6): Linear(in_features=5, out_features=1, bias=True)\n",
       ")>"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.share_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
